{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This box contains notes for myself\n"
     ]
    }
   ],
   "source": [
    "\"\"\"###################################################################\n",
    "Module to be loaded before running the script:\n",
    "- ANTs/git\n",
    "- qbatch/git\n",
    "\n",
    "###################################################################\"\"\"\n",
    "print \"This box contains notes for myself\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ====================================================================\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import glob #for wildcard matching\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# Setup directories, paths and filenames\n",
    "# - All paths must be ABSOLUTE!\n",
    "# - Refer to 'glob' manual to see the wildcard documentation for file-\n",
    "#        name matching\n",
    "# \n",
    "# ====================================================================\n",
    "\"\"\" Note on directory hierarchy for the input, reference and transform files \n",
    "parent_dir\n",
    "    |\n",
    "    +-- subj_dir_1 (as indicated by dir_wc)\n",
    "    |        |\n",
    "    |        +- file_of_interest (as indicated by file_wc)\n",
    "    |\n",
    "    +-- subj_dir_2 ...\n",
    "\"\"\"\n",
    "\n",
    "# =====================================\n",
    "# Input related\n",
    "# =====================================\n",
    "######## Input file paths ########\n",
    "# Path to the parent directory of the input files\n",
    "in_parent_dir='/data/chamal/projects/anthony/nmf_parcellation/cortical_tractMap/tract2voxel_probability_labels'\n",
    "# Wildcard / name of the individual subject directories\n",
    "in_dir_wc='[0-9][0-9][0-9][0-9][0-9][0-9]'\n",
    "# Wildcard / name of the input file\n",
    "in_file_wc='*.nii.gz'\n",
    "\n",
    "######## Reference / template file paths ########\n",
    "ref_parent_dir='/data/chamal/projects/anthony/HCP/raw_files/anthony'\n",
    "ref_dir_wc='[0-9][0-9][0-9][0-9][0-9][0-9]'\n",
    "ref_file_wc='HCP100unrelated-fa_template0_x07.nii.gz'\n",
    "\n",
    "######## Transform file paths ########\n",
    "tr_parent_dir='/data/chamal/projects/anthony/HCP/raw_files/anthony'\n",
    "tr_dir_wc='[0-9][0-9][0-9][0-9][0-9][0-9]'\n",
    "tr_file_wc='HCP100unrelated-fa_*_dt_fa*'\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# Output related\n",
    "# =====================================\n",
    "######## Output file paths ########\n",
    "out_parent_dir=''\n",
    " # NOTE:  output directory name will be the same as input directory (found via in_dir_wc)\n",
    "out_file_name=''\n",
    "\n",
    "######## Job document paths ########\n",
    "jobDoc_dir=''\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# antsApplyTransform related\n",
    "# =====================================\n",
    "DIMENSIONALITY='3'\n",
    "INTERPOLATION='GenericLabel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File initilization stats:\n",
      "\tTotal input files: 1467\n",
      "\tTotal reference files: 163\n",
      "\tTotal transform files: 200\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Save the filenames into lists\n",
    "# ====================================================================\n",
    "\n",
    "#### Initilize and save input files\n",
    "in_file_list=glob.glob(os.path.join(in_parent_dir,in_dir_wc,in_file_wc))\n",
    "in_file_list.sort()\n",
    "\n",
    "#### Initialize and save reference files\n",
    "ref_file_list=glob.glob(os.path.join(ref_parent_dir,ref_dir_wc,ref_file_wc))\n",
    "ref_file_list.sort()\n",
    "\n",
    "#### Initialize and save transform files\n",
    "tr_file_list=glob.glob(os.path.join(tr_parent_dir,tr_dir_wc,tr_file_wc))\n",
    "tr_file_list.sort()\n",
    "\n",
    "#Printout\n",
    "print \"File initilization stats:\"\n",
    "print \"\\tTotal input files: %d\" % len(in_file_list)\n",
    "print \"\\tTotal reference files: %d\" % len(ref_file_list)\n",
    "print \"\\tTotal transform files: %d\" % len(tr_file_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of subject ID read: 163\n",
      "Subjects deleted due to incomplete file paths: 63\n",
      "Remaining number of subjects stored in subj_files: 100\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Merge paths in lists into dictionaries for each subjet\n",
    "# - One does not need to use the above methods to get to this step;\n",
    "#       the below processing steps will work as long as this dict\n",
    "#       is initialized with the correct information\n",
    "# - Currently, the merging is done via string-matching subject ID\n",
    "#       in the file names\n",
    "# ====================================================================\n",
    "\n",
    "# Get a list of subject IDs\n",
    "subject_list_path = \"/data/chamal/projects/anthony/nmf_parcellation/cortical_tractMap/subj_list.txt\" \n",
    "IDs=[]\n",
    "f = open(subject_list_path, \"r\")\n",
    "for subj in f:\n",
    "    IDs.append(subj.strip())\n",
    "f.close()\n",
    "\n",
    "#Initilize the master dictionary to store subject filepaths\n",
    "subj_files = {}\n",
    "#Initialize all the sub-data structures within the dictionary\n",
    "for ID in IDs:\n",
    "    subj_files[ID] = {} #Initialize the individual subj dict\n",
    "    subj_files[ID]['in']=[] #Initialize list for input files\n",
    "    subj_files[ID]['tr']=[] #Initialize list of transform files\n",
    "\n",
    "#Store all of the input files data in their corresponding subject\n",
    "for i, filepath in enumerate(in_file_list):\n",
    "    subj_id = filepath.split('/')[-2] #Get the subject id\n",
    "    subj_files[subj_id]['in'].append(filepath)\n",
    "    \n",
    "for i, filepath in enumerate(ref_file_list):\n",
    "    subj_id = filepath.split('/')[-2]\n",
    "    subj_files[subj_id]['ref'] = filepath\n",
    "    \n",
    "for i, filepath in enumerate(tr_file_list):\n",
    "    subj_id = filepath.split('/')[-2]\n",
    "    subj_files[subj_id]['tr'].append(filepath)\n",
    "\n",
    "    \n",
    "#Delete the subjects that do not contain at least one of each\n",
    "subj_to_delete = []\n",
    "for subj_id in subj_files:\n",
    "    if len(subj_files[subj_id]['in']) == 0:\n",
    "        if subj_id not in subj_to_delete:\n",
    "            subj_to_delete.append(subj_id)\n",
    "    elif subj_files[subj_id]['ref'] == '':\n",
    "        if subj_id not in subj_to_delete:\n",
    "            subj_to_delete.append(subj_id)\n",
    "    elif len(subj_files[subj_id]['tr']) == 0:\n",
    "        if subj_id not in subj_to_delete:\n",
    "            subj_to_delete.append(subj_id)\n",
    "for subj_id in subj_to_delete:\n",
    "    del(subj_files[subj_id])\n",
    "    \n",
    "#TODO: this dictionary is not sorted!\n",
    "    \n",
    "\"\"\" \n",
    "# subj_files is the dictionary for all subsequent processing\n",
    "# see below for the variable hierarchy in this dictionary\n",
    "\n",
    "subj_files <type 'dict'>\n",
    "    |\n",
    "    +-- subj_files['100307'] <type 'dict'>\n",
    "    |        |\n",
    "    |        +-- subj_files['100307']['in'] <type 'list'>\n",
    "    |        |        |\n",
    "    |        |        +-- 100307_input_filepath_1 <type 'str'>\n",
    "    |        |        +-- 100307_input_filepath_2 <type 'str'>\n",
    "    |        |        |   ...\n",
    "    |        |\n",
    "    |        +-- subj_files['100307']['ref'] <type 'str'> (reference file)\n",
    "    |        |\n",
    "    |        +-- subj_files['100307']['tr'] <type 'list'>\n",
    "    |                 |\n",
    "    |                 +-- 100307_transform_filepath_1 <type 'str'>\n",
    "    |                 +-- 100307_transform_filepath_2 <type 'str'>\n",
    "    |                 |   ...?\n",
    "    |        \n",
    "    ...        \n",
    "\"\"\"\n",
    "\n",
    "#Print status lines\n",
    "print \"Total number of subject ID read: %d\" % len(IDs)\n",
    "print \"Subjects deleted due to incomplete file paths: %d\" % len(subj_to_delete)\n",
    "print \"Remaining number of subjects stored in subj_files: %d\" % len(subj_files)\n",
    "\n",
    "#TODO: initiate dataframe that will store all input and ouputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Iterate through each subject\n",
    "for subj_id in subj_files:\n",
    "    #Iterate through each input file (1 antsApplyTransform per input file)\n",
    "    \n",
    "    #antsApplyTransform command specifications\n",
    "    cmd_input['antsApplyTransforms','-d',DIMENSION,]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
