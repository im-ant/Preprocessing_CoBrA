{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This box contains notes for myself\n"
     ]
    }
   ],
   "source": [
    "\"\"\"###################################################################\n",
    "Module to be loaded before running the script:\n",
    "- ANTs/git\n",
    "- qbatch/git\n",
    "\n",
    "###################################################################\"\"\"\n",
    "print \"This box contains notes for myself\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ====================================================================\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import glob #for wildcard matching\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# Setup directories, paths and filenames\n",
    "# - All paths must be ABSOLUTE!\n",
    "# - Refer to 'glob' manual to see the wildcard documentation for file-\n",
    "#        name matching\n",
    "# \n",
    "# ====================================================================\n",
    "\"\"\" Note on directory hierarchy for the input, reference and transform files \n",
    "parent_dir\n",
    "    |\n",
    "    +-- subj_dir_1 (as indicated by dir_wc)\n",
    "    |        |\n",
    "    |        +- file_of_interest (as indicated by file_wc)\n",
    "    |\n",
    "    +-- subj_dir_2 ...\n",
    "\"\"\"\n",
    "\n",
    "# =====================================\n",
    "# Input related\n",
    "# =====================================\n",
    "######## Input file paths ########\n",
    "# Path to the parent directory of the input files\n",
    "in_parent_dir='/data/chamal/projects/anthony/nmf_parcellation/cortical_tractMap/tract2voxel_probability_labels/native_space'\n",
    "# Wildcard / name of the individual subject directories\n",
    "in_dir_wc='[0-9][0-9][0-9][0-9][0-9][0-9]'\n",
    "# Wildcard / name of the input file\n",
    "in_file_wc='*.nii.gz'\n",
    "\n",
    "######## Reference / template file paths ########\n",
    "ref_parent_dir='/data/chamal/projects/anthony/HCP/raw_files/anthony'\n",
    "ref_dir_wc='[0-9][0-9][0-9][0-9][0-9][0-9]'\n",
    "ref_file_wc='HCP100unrelated-fa_template0_x07.nii.gz'\n",
    "\n",
    "######## Transform file paths ########\n",
    "tr_parent_dir='/data/chamal/projects/anthony/HCP/raw_files/anthony'\n",
    "tr_dir_wc='[0-9][0-9][0-9][0-9][0-9][0-9]'\n",
    "tr_file_wc='HCP100unrelated-fa_*_dt_fa*'\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# Output related\n",
    "# =====================================\n",
    "######## Output file paths ########\n",
    "out_parent_dir='/data/chamal/projects/anthony/nmf_parcellation/cortical_tractMap/tract2voxel_probability_labels/model_space'\n",
    " # NOTE:  output directory will be made by the subject ID\n",
    "def generate_output_file_name(input_file_name): #Function to make output file name\n",
    "    name_parts = input_file_name.split('.')\n",
    "    name_parts[0] += '_modelSpace'\n",
    "    returnable = '.'.join(name_parts)\n",
    "    returnable = returnable.replace('_comp3_','_comp5') #TEMP FIX DELETE AFTER THIS RUN\n",
    "    return returnable \n",
    "\n",
    "######## Job document paths ########\n",
    "jobDoc_dir='/data/chamal/projects/anthony/qbatch_jobDocs'\n",
    "jobDoc_name='seg5_voxelizedTracts_native2Model_jobDoc.sh'\n",
    "jobList_name='autoSubmit_seg5_voxelizedTracts_native2Model_jobList.sub'\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# antsApplyTransform related\n",
    "# =====================================\n",
    "DIMENSIONALITY='3'\n",
    "INTERPOLATION='GenericLabel'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File initilization stats:\n",
      "\tTotal input files: 1467\n",
      "\tTotal reference files: 163\n",
      "\tTotal transform files: 200\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Save the filenames into lists\n",
    "# ====================================================================\n",
    "\n",
    "#### Initilize and save input files\n",
    "in_file_list=glob.glob(os.path.join(in_parent_dir,in_dir_wc,in_file_wc))\n",
    "in_file_list.sort()\n",
    "\n",
    "#### Initialize and save reference files\n",
    "ref_file_list=glob.glob(os.path.join(ref_parent_dir,ref_dir_wc,ref_file_wc))\n",
    "ref_file_list.sort()\n",
    "\n",
    "#### Initialize and save transform files\n",
    "tr_file_list=glob.glob(os.path.join(tr_parent_dir,tr_dir_wc,tr_file_wc))\n",
    "tr_file_list.sort()\n",
    "\n",
    "#Printout\n",
    "print \"File initilization stats:\"\n",
    "print \"\\tTotal input files: %d\" % len(in_file_list)\n",
    "print \"\\tTotal reference files: %d\" % len(ref_file_list)\n",
    "print \"\\tTotal transform files: %d\" % len(tr_file_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of subject ID read: 163\n",
      "Subjects deleted due to incomplete file paths: 63\n",
      "Remaining number of subjects stored in unordered_subj_files: 100\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Merge paths in lists into dictionaries for each subjet\n",
    "# - One does not need to use the above methods to get to this step;\n",
    "#       the below processing steps will work as long as this dict\n",
    "#       is initialized with the correct information\n",
    "# - Currently, the merging is done via string-matching subject ID\n",
    "#       in the file names\n",
    "# ====================================================================\n",
    "\n",
    "# Get a list of subject IDs\n",
    "subject_list_path = \"/data/chamal/projects/anthony/nmf_parcellation/cortical_tractMap/subj_list.txt\" \n",
    "IDs=[]\n",
    "f = open(subject_list_path, \"r\")\n",
    "for subj in f:\n",
    "    IDs.append(subj.strip())\n",
    "f.close()\n",
    "\n",
    "#Initilize the master dictionary to store subject filepaths\n",
    "unordered_subj_files = {}\n",
    "#Initialize all the sub-data structures within the dictionary\n",
    "for ID in IDs:\n",
    "    unordered_subj_files[ID] = {} #Initialize the individual subj dict\n",
    "    unordered_subj_files[ID]['in']=[] #Initialize list for input files\n",
    "    unordered_subj_files[ID]['tr']=[] #Initialize list of transform files\n",
    "\n",
    "#Store all of the input files data in their corresponding subject\n",
    "for i, filepath in enumerate(in_file_list):\n",
    "    subj_id = filepath.split('/')[-2] #Get the subject id\n",
    "    unordered_subj_files[subj_id]['in'].append(filepath)\n",
    "    \n",
    "for i, filepath in enumerate(ref_file_list):\n",
    "    subj_id = filepath.split('/')[-2]\n",
    "    unordered_subj_files[subj_id]['ref'] = filepath\n",
    "    \n",
    "for i, filepath in enumerate(tr_file_list):\n",
    "    subj_id = filepath.split('/')[-2]\n",
    "    unordered_subj_files[subj_id]['tr'].append(filepath)\n",
    "\n",
    "    \n",
    "#Delete the subjects that do not contain at least one of each\n",
    "subj_to_delete = []\n",
    "for subj_id in unordered_subj_files:\n",
    "    if len(unordered_subj_files[subj_id]['in']) == 0:\n",
    "        if subj_id not in subj_to_delete:\n",
    "            subj_to_delete.append(subj_id)\n",
    "    elif unordered_subj_files[subj_id]['ref'] == '':\n",
    "        if subj_id not in subj_to_delete:\n",
    "            subj_to_delete.append(subj_id)\n",
    "    elif len(unordered_subj_files[subj_id]['tr']) == 0:\n",
    "        if subj_id not in subj_to_delete:\n",
    "            subj_to_delete.append(subj_id)\n",
    "for subj_id in subj_to_delete:\n",
    "    del(unordered_subj_files[subj_id])\n",
    "    \n",
    "#TODO: this dictionary is not sorted!\n",
    "    \n",
    "\"\"\" \n",
    "# unordered_subj_files is the dictionary for all subsequent processing\n",
    "# see below for the variable hierarchy in this dictionary\n",
    "\n",
    "unordered_subj_files <type 'dict'>\n",
    "    |\n",
    "    +-- unordered_subj_files['100307'] <type 'dict'>\n",
    "    |        |\n",
    "    |        +-- unordered_subj_files['100307']['in'] <type 'list'>\n",
    "    |        |        |\n",
    "    |        |        +-- 100307_input_filepath_1 <type 'str'>\n",
    "    |        |        +-- 100307_input_filepath_2 <type 'str'>\n",
    "    |        |        |   ...\n",
    "    |        |\n",
    "    |        +-- unordered_subj_files['100307']['ref'] <type 'str'> (reference file)\n",
    "    |        |\n",
    "    |        +-- unordered_subj_files['100307']['tr'] <type 'list'>\n",
    "    |                 |\n",
    "    |                 +-- 100307_transform_filepath_1 <type 'str'>\n",
    "    |                 +-- 100307_transform_filepath_2 <type 'str'>\n",
    "    |                 |   ...?\n",
    "    |        \n",
    "    ...        \n",
    "\"\"\"\n",
    "\n",
    "#Print status lines\n",
    "print \"Total number of subject ID read: %d\" % len(IDs)\n",
    "print \"Subjects deleted due to incomplete file paths: %d\" % len(subj_to_delete)\n",
    "print \"Remaining number of subjects stored in unordered_subj_files: %d\" % len(unordered_subj_files)\n",
    "\n",
    "#TODO: initiate dataframe that will store all input and ouputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found output parent directory\n",
      "Found jobDoc directory\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Sort subjects, create PARENT output directories\n",
    "# ====================================================================\n",
    "\n",
    "# Sort the dictionary by key to create a ordered list of subjects\n",
    "import collections\n",
    "ord_subj_files = collections.OrderedDict(sorted(unordered_subj_files.items()))\n",
    "\n",
    "#Generate output parent directory if it is not present\n",
    "if not (os.path.isdir(out_parent_dir)):\n",
    "    os.mkdir(out_parent_dir)\n",
    "    print \"Made output parent directory\"\n",
    "else:\n",
    "    print \"Found output parent directory\"\n",
    "#Generate output jobDoc directory if it is not present\n",
    "if not (os.path.isdir(jobDoc_dir)):\n",
    "    os.mkdir(jobDoc_dir)\n",
    "    print \"Made jobDoc directory\"\n",
    "else:\n",
    "    print \"Found jobDoc directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# CUSTOMIZATIONS!\n",
    "# =====================================\n",
    "# QBatch related\n",
    "WAITTIME='10:00:00'\n",
    "CHUNKSIZE='9'\n",
    "PROCESSOR_PER_JOB='2' #Defalut = 1 \n",
    "\n",
    "#To submit or not to submit, that is the question\n",
    "SUBMIT = True\n",
    "\n",
    "# ====================================================================\n",
    "# Iterate through subjects to generate job\n",
    "# ====================================================================\n",
    "\n",
    "#Open jobDoc and jobList output streams\n",
    "jobDoc_outStream = open(os.path.join(jobDoc_dir,jobDoc_name),'w')\n",
    "jobList_outStream = open(os.path.join(jobDoc_dir,jobList_name),'w')\n",
    "\n",
    "#Write the jobDoc\n",
    "jobDoc_outStream.write(\"# Job doc generated on: %s\\n\" % time.strftime(\"%Y%d%m-%H%M\")) \n",
    "jobDoc_outStream.write(\"# Input files parent folder: %s\\n\" % in_parent_dir)\n",
    "jobDoc_outStream.write(\"# Reference files parent folder %s\\n\" % ref_parent_dir)\n",
    "jobDoc_outStream.write(\"# Transform files parent folder %s\\n\" % tr_parent_dir)\n",
    "jobDoc_outStream.write(\"# Output files parent folder: %s\\n#\\n\" % out_parent_dir)\n",
    "\n",
    "jobDoc_outStream.write(\"# antsApplyTransforms specs:\\n\")\n",
    "jobDoc_outStream.write(\"# \\tDimensionality: %s\\n\" % DIMENSIONALITY)\n",
    "jobDoc_outStream.write(\"# \\tInterpolation: %s\\n\"% INTERPOLATION)\n",
    "jobDoc_outStream.write(\"# Qbatch specs:\\n\")\n",
    "jobDoc_outStream.write(\"# \\tWaittime: %s\\n\" % WAITTIME)\n",
    "jobDoc_outStream.write(\"# \\tChunksize: %s\\n\" % CHUNKSIZE)\n",
    "jobDoc_outStream.write(\"# \\tProcessors per job: %s\\n\\n\" % PROCESSOR_PER_JOB)\n",
    "\n",
    "\n",
    "jobDoc_outStream.write(\"qbatch -w %s -c %s --ppj %s %s\\nexit\\n\\n\"%(WAITTIME,CHUNKSIZE,PROCESSOR_PER_JOB, os.path.join(jobDoc_dir,jobList_name)))\n",
    "\n",
    "\n",
    "#Iterate through subjects!\n",
    "for subj_id in ord_subj_files:\n",
    "    #Make the output subject directory, if it has not yet been made\n",
    "    output_subj_dir = os.path.join(out_parent_dir,subj_id)\n",
    "    if not (os.path.isdir(output_subj_dir)):\n",
    "        os.mkdir(output_subj_dir)\n",
    "    #Annotate jobdoc history\n",
    "    jobDoc_outStream.write(\"# Subject ID: %s\\n\" % subj_id)\n",
    "    \n",
    "    \n",
    "    #Iterate through each input file (1 antsApplyTransform per input file)\n",
    "    for input_file in ord_subj_files[subj_id]['in']:\n",
    "        #Generate output file path\n",
    "        output_filename = generate_output_file_name(input_file.split('/')[-1])\n",
    "        out_filepath = os.path.join(output_subj_dir,output_filename)\n",
    "        #antsApplyTransform command specifications\n",
    "        cmd_input=['antsApplyTransforms','-d',DIMENSIONALITY,\\\n",
    "                   '-i',input_file, '-r',ord_subj_files[subj_id]['ref'],\\\n",
    "                   '-o',out_filepath, '-n', INTERPOLATION]\n",
    "        #Add the transform files\n",
    "        for tr_file in ord_subj_files[subj_id]['tr']:\n",
    "            cmd_input += ['-t', tr_file]\n",
    "        \n",
    "        #Write out to the jobList file\n",
    "        jobList_outStream.write(' '.join(cmd_input)+'\\n')\n",
    "                \n",
    "        \n",
    "#Close the writing streams\n",
    "jobDoc_outStream.close()\n",
    "jobList_outStream.close()\n",
    "\n",
    "#Automatically submit if indicated\n",
    "if SUBMIT == True:\n",
    "    subprocess.call(['bash',os.path.join(jobDoc_dir,jobDoc_name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Ant's note to self\n",
    "- in directory: /data/chamal/projects/anthony/nmf_parcellation/cortical_tractMap/tract2voxel_probability_labels/native_space  \n",
    "    - the files have 'comp3' instead of 'comp5'. this is likely a mistake in the\n",
    "        previous step (in Chris' script). will need to fix in bash likely\n",
    "        to avoid confusion\n",
    "    - do note that this is fixed in the output file (i added extra\n",
    "        lines in the output file name generation to make it into comp5\n",
    "        instead\n",
    "\n",
    "\"\"\"\n",
    "print \"do the above pls\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
